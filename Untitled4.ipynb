{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzzduc/igtDTX4KA0n08eK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"kn7y0-sVeHrN"}},{"cell_type":"code","source":["!pip install tensorflow_text\n","import tensorflow_text as text\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn import preprocessing\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import ast"],"metadata":{"id":"ZxhdxWOcxRk5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689873096338,"user_tz":-60,"elapsed":92107,"user":{"displayName":"João Fonseca","userId":"18390905946977784863"}},"outputId":"7003f458-924a-4057-e8ee-c99cea1e997a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_text\n","  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.14.0)\n","Collecting tensorflow<2.14,>=2.13.0 (from tensorflow_text)\n","  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.56.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (3.8.0)\n","Collecting keras<2.14,>=2.13.1 (from tensorflow<2.14,>=2.13.0->tensorflow_text)\n","  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (16.0.0)\n","Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.16.0)\n","Collecting tensorboard<2.14,>=2.13 (from tensorflow<2.14,>=2.13.0->tensorflow_text)\n","  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<2.14,>=2.13.0->tensorflow_text)\n","  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (2.3.0)\n","Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow<2.14,>=2.13.0->tensorflow_text)\n","  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.2.2)\n","Installing collected packages: typing-extensions, tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow_text\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.7.1\n","    Uninstalling typing_extensions-4.7.1:\n","      Successfully uninstalled typing_extensions-4.7.1\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.3\n","    Uninstalling tensorboard-2.12.3:\n","      Successfully uninstalled tensorboard-2.12.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","Successfully installed keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow_text-2.13.0 typing-extensions-4.5.0\n"]}]},{"cell_type":"code","source":["import ast\n","import re"],"metadata":{"id":"mlRUefWUZs2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kvw_I8SiD0LV","executionInfo":{"status":"ok","timestamp":1689873127049,"user_tz":-60,"elapsed":30724,"user":{"displayName":"João Fonseca","userId":"18390905946977784863"}},"outputId":"f0d2b915-76f4-4997-8b95-fbf7d8732749"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Dar oversampling aos códigos que que aparecem menos de 9x"],"metadata":{"id":"x_yYEHYkxbWC"}},{"cell_type":"markdown","source":["## Ler os dados (Merged Food Parent Data)"],"metadata":{"id":"Df6ykfI36446"}},{"cell_type":"code","source":["#Read data\n","mfpd = pd.read_csv('/content/drive/My Drive/FoodEx2 project/Food Data/mfpd_2023-07-03_removed-entries.csv', header = 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V8mVlOPcYGmU","executionInfo":{"status":"ok","timestamp":1689173319706,"user_tz":-60,"elapsed":7981,"user":{"displayName":"João Fonseca","userId":"18390905946977784863"}},"outputId":"370c2990-9f12-4799-b831-9546a5456420"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-507f96628351>:2: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  mfpd = pd.read_csv('/content/drive/My Drive/FoodEx2 project/Food Data/mfpd_2023-07-03_removed-entries.csv', header = 0)\n"]}]},{"cell_type":"markdown","source":["## Preencher códigos em falta com aqueles da hierarquia acima"],"metadata":{"id":"FlArh8Zv-K54"}},{"cell_type":"code","source":["#Atribuir códigos parentais às entradas sem códigos\n","mfpd['2'].fillna(mfpd['1'], inplace=True)\n","print(mfpd['2'].isnull().sum())\n","\n","#Atribuir códigos parentais às entradas sem códigos\n","mfpd['3'].fillna(mfpd['2'], inplace=True)\n","print(mfpd['3'].isnull().sum())\n","\n","#Atribuir códigos parentais às entradas sem códigos\n","mfpd['4'].fillna(mfpd['3'], inplace=True)\n","print(mfpd['4'].isnull().sum())\n","\n","#Atribuir códigos parentais às entradas sem códigos\n","mfpd['5'].fillna(mfpd['4'], inplace=True)\n","print(mfpd['5'].isnull().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KDuOYoGMYgQI","executionInfo":{"status":"ok","timestamp":1689173321618,"user_tz":-60,"elapsed":1924,"user":{"displayName":"João Fonseca","userId":"18390905946977784863"}},"outputId":"173cb4f1-761c-4fce-b5d7-3fc1540750be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n","0\n","0\n"]}]},{"cell_type":"markdown","source":["## Oversample classes with less than 9 instances"],"metadata":{"id":"z7GeTpqI-gNa"}},{"cell_type":"code","source":["from imblearn.over_sampling import RandomOverSampler\n","\n","# Assuming you have a DataFrame called 'df' with a column 'class_label' representing the classes\n","df = mfpd[['index', 'tc', '5']]\n","df = df.rename(columns={'5': 'class_label'})\n","\n","# Count the occurrences of each class label\n","class_counts = df['class_label'].value_counts()\n","\n","# Identify classes with fewer than 9 occurrences\n","undersampled_classes = class_counts[class_counts < 9].index.tolist()\n","\n","# Apply oversampling to the undersampled classes\n","oversampler = RandomOverSampler(sampling_strategy={class_label: 9 for class_label in undersampled_classes}, random_state= 42)\n","oversampled_features, oversampled_labels = oversampler.fit_resample(df.drop('class_label', axis=1), df['class_label'])\n","\n","# Create the oversampled DataFrame\n","oversampled_df = pd.DataFrame(oversampled_features, columns=df.drop('class_label', axis=1).columns)\n","oversampled_df['class_label'] = oversampled_labels"],"metadata":{"id":"i1h2lTXJxlQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Criar amostras diferentes para treinar testar e validar"],"metadata":{"id":"qBwBC71DIZr5"}},{"cell_type":"code","source":["#Stratified Sampling\n","\n","from sklearn.model_selection import train_test_split\n","\n","def split_stratified_into_train_val_test(df_input, stratify_colname,\n","                                         frac_train, frac_val, frac_test,\n","                                         random_state):\n","\n","\n","    if frac_train + frac_val + frac_test != 1.0:\n","        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n","                         (frac_train, frac_val, frac_test))\n","\n","    if stratify_colname not in df_input.columns:\n","        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n","\n","    X = df_input # Contains all columns.\n","    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n","\n","    # Split original dataframe into train and temp dataframes.\n","    df_train, df_temp, y_train, y_temp = train_test_split(X,\n","                                                          y,\n","                                                          stratify=y,\n","                                                          test_size=(1.0 - frac_train),\n","                                                          random_state=random_state)\n","\n","    # Split the temp dataframe into val and test dataframes.\n","    relative_frac_test = frac_test / (frac_val + frac_test)\n","    df_val, df_test, y_val, y_test = train_test_split(df_temp,\n","                                                      y_temp,\n","                                                      stratify=y_temp,\n","                                                      test_size=relative_frac_test,\n","                                                      random_state=random_state)\n","\n","    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n","\n","    return df_train, df_val, df_test"],"metadata":{"id":"Z5I8IaYTXdfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train, df_val, df_test = split_stratified_into_train_val_test(oversampled_df, 'class_label', 0.6, 0.2, 0.2, 42)"],"metadata":{"id":"T1C5Pz4ZXscK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save data train, test and val"],"metadata":{"id":"kBJqvHm-wn2E"}},{"cell_type":"code","source":["#Save Data\n","#df_train.to_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_train_sampled.csv')"],"metadata":{"id":"B38YuKzGukaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df_val.to_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_val.csv')"],"metadata":{"id":"0GuY9Bbmu5yD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df_test.to_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_test.csv')"],"metadata":{"id":"rsL534T8wJA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sampling train data"],"metadata":{"id":"SaGUxoJF4jSD"}},{"cell_type":"markdown","source":["### Read train data"],"metadata":{"id":"xUn2zFysyr18"}},{"cell_type":"code","source":["#Read data\n","df_train = pd.read_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_train.csv', header = 0)"],"metadata":{"id":"ULU0378Xyq7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample"],"metadata":{"id":"S8gScT8DyvC_"}},{"cell_type":"code","source":["df_train_nodup = df_train[['tc','class_label']].drop_duplicates().reset_index(drop = True)"],"metadata":{"id":"hXKksuXfh6fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_sampled = pd.DataFrame(columns = ['tc','class_label'])\n","for label in df_train_nodup['class_label'].unique():\n","  sample_size = len(df_train_nodup.loc[df_train_nodup['class_label'] == label]) * 220\n","  data = df_train_nodup.loc[df_train_nodup['class_label'] == label].sample(sample_size, replace = True, random_state = 42).reset_index(drop=True)\n","  df_train_sampled = pd.concat([df_train_sampled, data])"],"metadata":{"id":"kpTB6Oz3mgJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_sampled = df_train_sampled.reset_index(drop=True)"],"metadata":{"id":"PFhEAXwKqE67"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Criar embeddings (pesquisar melhor sobre este modelo)"],"metadata":{"id":"RztrchACTH52"}},{"cell_type":"code","source":["preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n","encoder_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/2'"],"metadata":{"id":"Ygj4TQWrz3Q4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load the preprocessing function in the url\n","bert_preprocess_model = hub.KerasLayer(preprocess_url)\n","\n","#load bert encoder\n","bert_encoder = hub.KerasLayer(encoder_url)"],"metadata":{"id":"psLIMSCZ1ya8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeW7Q-bKd6uy"},"outputs":[],"source":["#Bert model - processing and embedding\n","def get_sentence_embeding(sentences):\n","    preprocessed_text = bert_preprocess_model(sentences)\n","    return bert_encoder(preprocessed_text)['pooled_output']"]},{"cell_type":"code","source":["# Initialize the output DataFrame or file for writing transformed data\n","output_file = 'df_train_nodup_embedded.csv'  # Adjust the output file name as needed\n","\n","increment = 100  # Increment size\n","df = df_train_nodup\n","column = 'tc'\n","for start in tqdm(range(0, len(df), increment), total = len(df)/increment):\n","    end = start + increment\n","    chunk = df.iloc[start:end]  # Access rows in increments\n","    # Preprocess and transform the chunk of data\n","    transformed_chunk = get_sentence_embeding(chunk[column].values)\n","    # Write the transformed data to disk\n","    pd.DataFrame({'Column': transformed_chunk.numpy().tolist()}).to_csv(output_file, mode='a', header=False, index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hW0By5MXLixL","executionInfo":{"status":"ok","timestamp":1689174333131,"user_tz":-60,"elapsed":361193,"user":{"displayName":"João Fonseca","userId":"18390905946977784863"}},"outputId":"26101d47-6fc8-40b2-94a6-2a3fd840c531"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 123/123.9 [05:59<00:03,  3.39s/it]/usr/local/lib/python3.10/dist-packages/tqdm/std.py:524: TqdmWarning: clamping frac to range [0, 1]\n","  full_bar = Bar(frac,\n","100%|██████████| 124/123.9 [06:01<00:00,  2.92s/it]\n"]}]},{"cell_type":"code","source":["#Read data\n","df_train_nodup_embedded = pd.read_csv('/content/df_train_nodup_embedded.csv', header=None, names=['embed'])"],"metadata":{"id":"yS57_qjtofKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_nodup_embedded_temp = pd.merge(df_train_nodup_embedded.reset_index(),\n","                                        df_train_nodup.reset_index(),\n","                                        on = 'index')"],"metadata":{"id":"WRq9WKu2ohCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save Data\n","#df_train_nodup_embedded_temp.to_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_train_nodup_embedded.csv')"],"metadata":{"id":"JoOpuYvlVj4o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Attributing embeddings to food descriptions"],"metadata":{"id":"VKUEenyZe2Ft"}},{"cell_type":"code","source":["#Read training data data\n","df_train_sampled = pd.read_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_train_sampled.csv', header = 0)"],"metadata":{"id":"tkHcLkL1G_Wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df_train_sampled)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSO6oXBTkpNH","executionInfo":{"status":"ok","timestamp":1689777256668,"user_tz":-60,"elapsed":249,"user":{"displayName":"João Fonseca","userId":"18390905946977784863"}},"outputId":"d6f49bd2-0d1a-4525-a891-d7b8fca48f6c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2725800"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#Read embedded data\n","df_train_nodup_embedded_temp = pd.read_csv('/content/drive/My Drive/FoodEx2 project/Food Data/df_train_nodup_embedded.csv', header = 0)"],"metadata":{"id":"DFBLxjLTHTrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Codify class labels in numeric values\n","le = preprocessing.LabelEncoder()\n","le.fit(df_train_sampled['class_label'].unique())\n","df_train_sampled['class_label_nr'] = le.transform(df_train_sampled['class_label'])"],"metadata":{"id":"8e1vUigrbya5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the output DataFrame or file for writing transformed data\n","output_file = 'df_train_embedded.csv'  # Adjust the output file name as needed\n","\n","df_one = df_train_sampled[['tc', 'class_label', 'class_label_nr']]\n","column = 'tc'\n","df_two = df_train_nodup_embedded_temp[['tc', 'embed']].drop_duplicates()"],"metadata":{"id":"7UJf_KmJfy6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Appending embbedings to sampeld train data\n","df_one['embed'] = df_one[column].map(df_two.set_index(column)['embed'])"],"metadata":{"id":"jtmGgUG2fB8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save de dataframe in disk\n","df_one.to_csv(output_file, index = False)"],"metadata":{"id":"5X_1Zbq5hINN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Delete dataframe from ram\n","del df_one"],"metadata":{"id":"ExWMm7poCLVJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create an iterator to load the data to train the model*"],"metadata":{"id":"HSyLiatzCnkL"}},{"cell_type":"code","source":["df_train_sampled = tf.data.experimental.make_csv_dataset('/content/df_train_embedded.csv',\n","                                                        batch_size = 32,\n","                                                        select_columns = ['embed', 'class_label_nr'],\n","                                                        label_name = 'class_label_nr',\n","                                                        num_epochs=1,\n","                                                        shuffle=True,\n",")\n"],"metadata":{"id":"6-jXur2fSsaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def preprocess_string(features, labels):\n","    features_copy = {}\n","    features_copy['Input'] = tf.strings.regex_replace(features['embed'], '[\\[\\]]', '')\n","    features_copy['Input'] = tf.strings.split(features_copy['Input'], sep=',').to_tensor()\n","    features_copy['Input'] = tf.strings.to_number(features_copy['Input'], out_type=tf.float32)\n","    return features_copy, labels"],"metadata":{"id":"Ml1YeDnK4ADI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train_sampled = df_train_sampled.repeat()\n","df_train_sampled = df_train_sampled.map(preprocess_string)"],"metadata":{"id":"EN40o-diKEG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iterator = iter(df_train_sampled)"],"metadata":{"id":"birDmGBGNxLn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Create a sequential model"],"metadata":{"id":"a3CAzN73NyZt"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(tf.keras.Input(shape=(128,), name = 'Input'))\n","model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(1575, activation = 'softmax', name = 'Code'))"],"metadata":{"id":"YdbhJgr-_AIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"],"metadata":{"id":"YwWNDtAvADtT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","batch_size_learn = 32\n","total_samples = 2725800\n","steps_per_epoch = total_samples // batch_size_learn\n","\n","# Create a ModelCheckpoint callback to save the model with the best accuracy\n","checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model_2023-07-20.h5', monitor='accuracy', save_best_only=True, mode='max', verbose=1)\n","\n","history = model.fit(iterator,\n","                    epochs = num_epochs,\n","                    verbose = 1,\n","                    batch_size = batch_size_learn,\n","                    steps_per_epoch = steps_per_epoch,\n","                    callbacks=[checkpoint])"],"metadata":{"id":"uVPACC3VAadt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save Model\n","#model.save('/content/drive/My Drive/FoodEx2 project/Food Data/Models/bert_3L_128.h5')"],"metadata":{"id":"lUMrsJWQy_cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save class discretization values\n","#np.savetxt('/content/drive/My Drive/FoodEx2 project/Food Data/Models/classes_num.csv', le.classes_, fmt='%s')"],"metadata":{"id":"mXACYTRf0nMh"},"execution_count":null,"outputs":[]}]}